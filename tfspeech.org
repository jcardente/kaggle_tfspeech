#+Title: TensorFlow Speech Recognition Challenge


* Info

Contest url:
https://www.kaggle.com/c/tensorflow-speech-recognition-challenge#evaluation

Useful links
- https://lo.calho.st/projects/generating-a-spectrogram-using-numpy/
- https://kratzert.github.io/2017/06/15/example-of-tensorflows-new-input-pipeline.html
- http://ischlag.github.io/2016/11/07/tensorflow-input-pipeline-for-large-datasets/
- http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html

* Notes
** <2017-12-15 Fri> Pre-processing audio

Found this post on computing a spectogram from a WAV file in python,
https://lo.calho.st/projects/generating-a-spectrogram-using-numpy/

Researching merits of scaling audio to have a consistent volume or other normalization approach.
- https://www.learndigitalaudio.com/normalize-audio
- https://towardsdatascience.com/audio-processing-in-tensorflow-208f1a4103aa
** <2017-12-17 Sun> Refactoring code

Experimenting with proprocessing code

#+BEGIN_SRC python
  import preprocessing as prep

  fname = '../data/train/audio/yes/57cb3575_nohash_0.wav'
  #fname = '../data/train/audio/dog/00b01445_nohash_0.wav'

  w = prep.WavWrapper(fname)

  framerate       = w._framerate
  overlapRate     = 4
  framesPerWindow = 512 #int(nframes/nwindowsPerSec)

  Y = []
  for x in prep.overlappedWindow(w,framesPerWindow, overlapRate):
      Y.append(prep.doFFT(x))

  Y = np.column_stack(Y)

#+END_SRC


#+BEGIN_SRC python

  def plotSpectrogram(Y, framerate, framesPerWindow, overlapRate):
      f = np.arange(framesPerWindow/2, dtype=np.float) * framerate / framesPerWindow
      t = np.arange(0, Y.shape[1], dtype=np.float) * framesPerWindow / framerate / overlapRate

      # PLOT THE SPECTOGRAM
      ax = plt.subplot(111)

      plt.pcolormesh(t, f, Y, vmin=-120, vmax=0)
      plt.yscale('symlog', linthreshy=100, linscaley=0.25)
      ax.yaxis.set_major_formatter(matplotlib.ticker.ScalarFormatter())

      plt.xlim(0, t[-1])
      plt.ylim(0, f[-1])

      plt.xlabel("Time (s)")
      plt.ylabel("Frequency (Hz)")

      cbar = plt.colorbar()
      cbar.set_label("Intensity (dB)")

      plt.show()
#+END_SRC


New code to read in entire file, makes life easier

#+BEGIN_SRC python

  def readWavFile(fname):
          w = wave.open(fname, 'r')
          framerate = w.getframerate()
          sampwidth = w.getsampwidth()
          nchannels = w.getnchannels()
          sampdtype = np.int8 if sampwidth==1 else np.int16

          data = w.readframes(w.getnframes())
          data = np.fromstring(data, dtype = sampdtype)
          data = data.astype(float) / np.max(np.abs(data))
          data = np.reshape(data,(len(data) // nchannels, nchannels))

          w.close()
          return [data, framerate]


  def overlappedWindowIter(data, windowSize, overlapRate):
          start   = 0
          stepsz  = windowSize//overlapRate
          weights = np.array(hanning(windowSize)).reshape(windowSize,1)
          while True:
                  window = data[start:(start+windowSize),]
                  if len(window) != windowSize:
                          return
                  yield window * weights
                  start += stepsz

  def doFFT(data):
      mindB = np.power(10.0, -120/20)  # Lowest signal level in dB
      y = np.fft.rfft(data)
      y = y[:len(data)//2]
      y = np.absolute(y) * 2.0 / len(data)
      #y = y / np.power(2.0, 8*nsampwidth - 1)
      #y = y / np.sum(y)
      y = y / np.max(y)
      y = 20 * np.log10(y.clip(mindB)) # clip before log to avoid log10 0 errors
      return y

  def calcSpectogram(fname, windowSize, overlapRate):
     data, framerate = readWavFile(fname)
     Y = [doFFT(x) for x in overlappedWindowIter(data, windowSize, overlapRate)]
     return np.column_stack(Y), framerate

#+END_SRC


#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker
import preprocessing as prep

fname = '../data/train/audio/yes/57cb3575_nohash_0.wav'
framesPerWindow = 512
overlapRate     = 4

data, framerate = prep.calcSpectrogram(fname, framesPerWindow, overlapRate)
#+END_SRC


#+BEGIN_SRC python
from os import listdir
from os.path import isfile, join

path = '../data/train/audio/marvin'
yesfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f))]

framesPerWindow = 256
overlapRate = 4
fname = yesfiles[np.random.randint(len(yesfiles))]
spect, framerate = prep.calcSpectrogram(fname, framesPerWindow, overlapRate)
plotSpectogram(spect, framerate, framesPerWindow, overlapRate)

#+END_SRC

** <2017-12-19 Tue> Checking spectrogram valid over data

#+BEGIN_SRC python
import util
import numpy as np

audioPath = '../data/train/audio'
labels, datasets = util.splitTrainData(audioPath, 0)
noutputs = len(labels)


assert( len(datasets['validation']) ==0) and len(datasets['testing']) == 0)
for elem in datasets['training']:
    # parse one audio file to get types and dimensions
    data, _ = util.readWavFile(elem[1])
    if np.any(np.isnan(data)):
       print("File {} has nans".format(elem[1]))



for elem in datasets['training']:
    # parse one audio file to get types and dimensions
    try:
        tmpspectro, _ = util.calcSpectrogram(elem[1], 512, 4)
    except AssertionError:
        print("Problem with file " + elem[1])


#+END_SRC

Found these files have nans in wav data:
../data/train/audio/bird/3e7124ba_nohash_0.wav

** <2017-12-20 Wed> LSTM experiment

#+BEGIN_SRC python
  import tensorflow as tf
  data = np.random.random((4,5,10))

  tf.reset_default_graph()

  nhidden = 2
  input_data = tf.placeholder(tf.float32, [None, 5, 10])
  batch_data = tf.unstack(tf.transpose(input_data, perm=[1,0,2]))
  lstm_cell  = tf.contrib.rnn.LSTMCell(num_units=nhidden)
  output_seqs, states = tf.contrib.rnn.static_rnn(lstm_cell, batch_data, dtype=tf.float32)
  flat_states = tf.stack(states, axis=1)
  flat_states = tf.reshape(flat_states, [-1,2*nhidden])

  init_op = tf.global_variables_initializer()
  sess = tf.InteractiveSession()

  sess.run(init_op)
  fo, fs, flat = sess.run([output_seqs,states, flat_states], feed_dict={input_data: data})

#+END_SRC

The above produces an output state vector that is has a single dimension per batch
** <2017-12-21 Thu> MFCC experiment


Experimenting with calculating MFCC's for the speech signature

#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt
from python_speech_features import mfcc
import util

fname = '../data/train/audio/sheila/1fe4c891_nohash_1.wav'
data, samprate = util.readWavFile(fname)
xx = util.doMFCC(data, samprate)

t = np.arange(0.01,1,0.01)
f = np.arange(xx.shape[1])

plt.pcolormesh(t,f, np.transpose(xx))
#+END_SRC
