#+Title: TensorFlow Speech Recognition Challenge


* Info

Contest url:
https://www.kaggle.com/c/tensorflow-speech-recognition-challenge#evaluation

Useful links
- https://lo.calho.st/projects/generating-a-spectrogram-using-numpy/
- https://kratzert.github.io/2017/06/15/example-of-tensorflows-new-input-pipeline.html
- http://ischlag.github.io/2016/11/07/tensorflow-input-pipeline-for-large-datasets/
- http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html
- https://stackoverflow.com/questions/46917588/restoring-a-tensorflow-model-that-uses-iterators
- http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
- https://www.tensorflow.org/versions/master/tutorials/audio_recognition
- http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf

* Notes
** <2017-12-15 Fri> Pre-processing audio

Found this post on computing a spectogram from a WAV file in python,
https://lo.calho.st/projects/generating-a-spectrogram-using-numpy/

Researching merits of scaling audio to have a consistent volume or other normalization approach.
- https://www.learndigitalaudio.com/normalize-audio
- https://towardsdatascience.com/audio-processing-in-tensorflow-208f1a4103aa
** <2017-12-17 Sun> Refactoring code

Experimenting with proprocessing code

#+BEGIN_SRC python
  import preprocessing as prep

  fname = '../data/train/audio/yes/57cb3575_nohash_0.wav'
  #fname = '../data/train/audio/dog/00b01445_nohash_0.wav'

  w = prep.WavWrapper(fname)

  framerate       = w._framerate
  overlapRate     = 4
  framesPerWindow = 512 #int(nframes/nwindowsPerSec)

  Y = []
  for x in prep.overlappedWindow(w,framesPerWindow, overlapRate):
      Y.append(prep.doFFT(x))

  Y = np.column_stack(Y)

#+END_SRC


#+BEGIN_SRC python

  def plotSpectrogram(Y, framerate, framesPerWindow, overlapRate):
      f = np.arange(framesPerWindow/2, dtype=np.float) * framerate / framesPerWindow
      t = np.arange(0, Y.shape[1], dtype=np.float) * framesPerWindow / framerate / overlapRate

      # PLOT THE SPECTOGRAM
      ax = plt.subplot(111)

      plt.pcolormesh(t, f, Y, vmin=-120, vmax=0)
      plt.yscale('symlog', linthreshy=100, linscaley=0.25)
      ax.yaxis.set_major_formatter(matplotlib.ticker.ScalarFormatter())

      plt.xlim(0, t[-1])
      plt.ylim(0, f[-1])

      plt.xlabel("Time (s)")
      plt.ylabel("Frequency (Hz)")

      cbar = plt.colorbar()
      cbar.set_label("Intensity (dB)")

      plt.show()
#+END_SRC


New code to read in entire file, makes life easier

#+BEGIN_SRC python

  def readWavFile(fname):
          w = wave.open(fname, 'r')
          framerate = w.getframerate()
          sampwidth = w.getsampwidth()
          nchannels = w.getnchannels()
          sampdtype = np.int8 if sampwidth==1 else np.int16

          data = w.readframes(w.getnframes())
          data = np.fromstring(data, dtype = sampdtype)
          data = data.astype(float) / np.max(np.abs(data))
          data = np.reshape(data,(len(data) // nchannels, nchannels))

          w.close()
          return [data, framerate]


  def overlappedWindowIter(data, windowSize, overlapRate):
          start   = 0
          stepsz  = windowSize//overlapRate
          weights = np.array(hanning(windowSize)).reshape(windowSize,1)
          while True:
                  window = data[start:(start+windowSize),]
                  if len(window) != windowSize:
                          return
                  yield window * weights
                  start += stepsz

  def doFFT(data):
      mindB = np.power(10.0, -120/20)  # Lowest signal level in dB
      y = np.fft.rfft(data)
      y = y[:len(data)//2]
      y = np.absolute(y) * 2.0 / len(data)
      #y = y / np.power(2.0, 8*nsampwidth - 1)
      #y = y / np.sum(y)
      y = y / np.max(y)
      y = 20 * np.log10(y.clip(mindB)) # clip before log to avoid log10 0 errors
      return y

  def calcSpectogram(fname, windowSize, overlapRate):
     data, framerate = readWavFile(fname)
     Y = [doFFT(x) for x in overlappedWindowIter(data, windowSize, overlapRate)]
     return np.column_stack(Y), framerate

#+END_SRC


#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker
import preprocessing as prep

fname = '../data/train/audio/yes/57cb3575_nohash_0.wav'
framesPerWindow = 512
overlapRate     = 4

data, framerate = prep.calcSpectrogram(fname, framesPerWindow, overlapRate)
#+END_SRC


#+BEGIN_SRC python
from os import listdir
from os.path import isfile, join

path = '../data/train/audio/marvin'
yesfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f))]

framesPerWindow = 256
overlapRate = 4
fname = yesfiles[np.random.randint(len(yesfiles))]
spect, framerate = prep.calcSpectrogram(fname, framesPerWindow, overlapRate)
plotSpectogram(spect, framerate, framesPerWindow, overlapRate)

#+END_SRC

** <2017-12-19 Tue> Checking spectrogram valid over data

#+BEGIN_SRC python
import util
import numpy as np

audioPath = '../data/train/audio'
labels, datasets = util.splitTrainData(audioPath, 0)
noutputs = len(labels)


assert( len(datasets['validation']) ==0) and len(datasets['testing']) == 0)
for elem in datasets['training']:
    # parse one audio file to get types and dimensions
    data, _ = util.readWavFile(elem[1])
    if np.any(np.isnan(data)):
       print("File {} has nans".format(elem[1]))



for elem in datasets['training']:
    # parse one audio file to get types and dimensions
    try:
        tmpspectro, _ = util.calcSpectrogram(elem[1], 512, 4)
    except AssertionError:
        print("Problem with file " + elem[1])


#+END_SRC

Found these files have nans in wav data:
../data/train/audio/bird/3e7124ba_nohash_0.wav

** <2017-12-20 Wed> LSTM experiment

#+BEGIN_SRC python
  import tensorflow as tf
  data = np.random.random((4,5,10))

  tf.reset_default_graph()

  nhidden = 2
  input_data = tf.placeholder(tf.float32, [None, 5, 10])
  batch_data = tf.unstack(tf.transpose(input_data, perm=[1,0,2]))
  lstm_cell  = tf.contrib.rnn.LSTMCell(num_units=nhidden)
  output_seqs, states = tf.contrib.rnn.static_rnn(lstm_cell, batch_data, dtype=tf.float32)
  flat_states = tf.stack(states, axis=1)
  flat_states = tf.reshape(flat_states, [-1,2*nhidden])

  init_op = tf.global_variables_initializer()
  sess = tf.InteractiveSession()

  sess.run(init_op)
  fo, fs, flat = sess.run([output_seqs,states, flat_states], feed_dict={input_data: data})

#+END_SRC

The above produces an output state vector that is has a single dimension per batch
** <2017-12-21 Thu> MFCC experiment


Experimenting with calculating MFCC's for the speech signature

#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt
from python_speech_features import mfcc
import util

fname = '../data/train/audio/sheila/1fe4c891_nohash_1.wav'
data, samprate = util.readWavFile(fname)
xx = util.doMFCC(data, samprate)

t = np.arange(0.01,1,0.01)
f = np.arange(xx.shape[1])

plt.pcolormesh(t,f, np.transpose(xx))
#+END_SRC
** <2017-12-27 Wed> Reviewing TFlow example

Looking over the tensorflow speech command example
https://www.tensorflow.org/versions/master/tutorials/audio_recognition


Input data augmentation
- Uknown: draw 10% from labeled data outside list of target words
- Silence: draw 10% from background noise samples
- Background noise: mix background noise into training
  samples. Randomly adjust volume (0 to 1) and mix frequency.
- Time shifting: randomly shift training samples by up to 100ms, pad
  space with zeros


File that defines various models
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py

Code that prepares the model parameters

#+BEGIN_SRC python
def prepare_model_settings(label_count, sample_rate, clip_duration_ms,
                           window_size_ms, window_stride_ms,
                           dct_coefficient_count):
  """Calculates common settings needed for all models.
  Args:
    label_count: How many classes are to be recognized.
    sample_rate: Number of audio samples per second.
    clip_duration_ms: Length of each audio clip to be analyzed.
    window_size_ms: Duration of frequency analysis window.
    window_stride_ms: How far to move in time between frequency windows.
    dct_coefficient_count: Number of frequency bins to use for analysis.
  Returns:
    Dictionary containing common settings.
  """
  desired_samples = int(sample_rate * clip_duration_ms / 1000)
  window_size_samples = int(sample_rate * window_size_ms / 1000)
  window_stride_samples = int(sample_rate * window_stride_ms / 1000)
  length_minus_window = (desired_samples - window_size_samples)
  if length_minus_window < 0:
    spectrogram_length = 0
  else:
    spectrogram_length = 1 + int(length_minus_window / window_stride_samples)
  fingerprint_size = dct_coefficient_count * spectrogram_length
  return {
      'desired_samples': desired_samples,
      'window_size_samples': window_size_samples,
      'window_stride_samples': window_stride_samples,
      'spectrogram_length': spectrogram_length,
      'dct_coefficient_count': dct_coefficient_count,
      'fingerprint_size': fingerprint_size,
      'label_count': label_count,
      'sample_rate': sample_rate,
  }
#+END_SRC

default values

| Parameter             |                                    Default |
|-----------------------+--------------------------------------------|
| Background Volume     |                                        0.1 |
| Background Freq       |                                        0.8 |
| Silence Percentage    |                                       10.0 |
| Unknown Percentage    |                                       10.0 |
| Time shift max        |                                    100.0ms |
| Test Percentage       |                                         10 |
| Validation Percentage |                                         10 |
| Sample Rate           |                                      16000 |
| Clip duration         |                                     1000ms |
| Window Size           |                                       30ms |
| Window Stride         |                                       10ms |
| DCT Coefficient Count |                                         40 |
| Training steps        |                                 15000,3000 |
| Eval interval         |                                        400 |
| Learning rate         |                              0.001, 0.0001 |
| Batch size            |                                        100 |
| Wanted words          | 'yes,no,up,down,left,right,on,off,stop,go' |
| Save interval         |                                        100 |
| Model architecture    |                                       conv |
| Check nans            |                                      False |



** <2018-01-03 Wed> Augmentation experiments

Testing augmentation code

#+BEGIN_SRC python
maxShiftSamps = int(16000/10)
fname = '../data/train/audio/off/483e2a6f_nohash_2.wav'

data, sr = util.readWavFile(fname)
shifted  = util.dataTrainShift(data, maxShiftSamps)

def plotshifted(data, shifted):
    plt.subplot(2,1,1)
    plt.plot(data)
    plt.subplot(2,1,2)
    plt.plot(shifted)
    plt.show()

backgrounds = util.dataBackgroundLoad('../data/train/audio', PARAMS)


 for i in range(20):
     bgmix = util.dataBackgroundMixin(data, backgrounds, PARAMS)
     sd.play(bgmix, 16000)
     sd.wait()


#+END_SRC
